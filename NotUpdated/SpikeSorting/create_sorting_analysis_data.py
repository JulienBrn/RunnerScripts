from pathlib import Path
from pydantic import Field, BaseModel
from script2runner import CLI
from typing import List, Literal, Dict, ClassVar, Annotated
import shutil, datetime as dt
import yaml
import re
import subprocess, sys
import time


start_path_patterns = ["/media/filer2/T4b/", "/media/filer2/T4/", "/media/t4user/data1/", "/media/BigNAS/", "/home/t4user/"]
def get_file_pattern_from_suffix_list(start_path_patterns, suffixes):
    def mk_or_pattern(options):
        return '(('+ ")|(".join([re.escape(opt) for opt in options])+ '))'
    return '^'+ mk_or_pattern(start_path_patterns)+r'[^\\]*'+ mk_or_pattern(suffixes) + "$"
    
class AnalyzerMainParam(BaseModel):
    max_spikes_per_unit: None | int = None

class Args(CLI):
    """
        - Goal: Creates analysis data of the recording. 
        - Technique: Uses spike interface sorting analyzer and custom tools. 
        - Formats: Input and output formats are in the .zarr formats generated by spike interface.
    """
    recording_path: Annotated[Path, Field(
        description="Path to the folder containing the (usually preprocessed) recording",
        examples=["/media/t4user/data1/Data/SpikeSorting/...si.zarr"], 
        json_schema_extra=dict(pattern=get_file_pattern_from_suffix_list(["/media/t4user/data1/"], [".si.zarr"]))
    )]
    spikeinterface_analyzer_path: Annotated[Path, Field(
        examples=["/media/filer2/T4b/Temporary/....si.zarr"], 
        description="Location of the analyzer", 
        json_schema_extra=dict(pattern=get_file_pattern_from_suffix_list(start_path_patterns, [".si.zarr"]))
    )]
    output_path: Annotated[Path, Field(
        examples=["/media/filer2/T4b/Temporary/....xr.zarr"], 
        description="Location of the output xarray data for plotting", 
        json_schema_extra=dict(pattern=get_file_pattern_from_suffix_list(start_path_patterns, [".xr.zarr"]))
    )]

    overwrite: Literal["yes", "no"] = Field(default="no", description="Whether to overwrite and continue if output exists")
    _run_info: ClassVar = dict(conda_env="ssnew", uses_gpu=False)
    
a = Args()

if a.output_path.exists():
    if a.overwrite =="no":
        print(f"{a.output_path} already exists")
        exit(2)
    else:
        shutil.rmtree(a.output_path)
        a.output_path.parent.mkdir(exist_ok=True, parents=True)


import spikeinterface as si
import yaml
import numpy as np, xarray as xr, pandas as pd
from xhistogram.xarray import histogram
import tqdm.auto as tqdm

dimensions = dict(principal_components=["rnd_spike", "pc", "sparse_channel"],
                  noise_levals=["channel"],
                  random_spikes=["rnd_spike"],
                  unit_locations=["unit", "space_ax"],
                  spike_amplitudes=["spike"],
                  templates=["unit", "wf_t", "channel"],
                  waveforms=["rnd_spike", "wf_t", "sparse_channel"],
                #   spike_locations=["spike"],
                  template_metrics=["unit", "template_metric"],
                  template_similarity=["unit", "unit2"],
                  quality_metrics=["unit", "quality_metric"],
)

def spike_interface_to_xarray(analyzer: si.SortingAnalyzer, sorting: si.BaseSorting, recording: si.BaseRecording):
   extensions = analyzer.get_loaded_extension_names()
   d = xr.Dataset()
   ssv = sorting.to_spike_vector()
   d["spike_time"] = xr.DataArray(ssv["sample_index"]/analyzer.sampling_frequency, dims="spike")
   d["spike_unit_index"] = xr.DataArray(ssv["unit_index"], dims="spike")
   
   for ext in extensions:
      s = analyzer.get_extension(ext)
      data = s.get_data()
      if ext in dimensions:
            if ext in ["template_metrics", "quality_metrics"]:
                data = data.astype(float)
            d[ext] = xr.DataArray(data, dims=dimensions[ext])
   if "correlograms" in extensions:
      corr, corr_bins = analyzer.get_extension("correlograms").get_data()
      d["correlogram"] = xr.DataArray(corr, dims=["unit", "unit2", "corr_t"])
      d["corr_t"] = (corr_bins[1:] + corr_bins[: -1])/2000
   if "isi_histograms" in extensions:
      isi, isi_bins = analyzer.get_extension("isi_histograms").get_data()
      d["isi_hist"] = xr.DataArray(isi, dims=["unit", "isi_t"])
      d["isi_t"] = (isi_bins[1:] + isi_bins[: -1])/2000
   def get_sparsity(unit):
      ids = analyzer.sparsity.unit_id_to_channel_ids[unit.item()]
      return np.pad(ids, mode="constant", constant_values="", pad_width=(0, d["sparse_channel"].size - len(ids)))
   d["sparse_channel_name"] = xr.DataArray([get_sparsity(unit) for unit in d["unit"]], dims=["unit", "sparse_channel"])
   d = d.sel(sparse_channel=(d["sparse_channel_name"]!="").any("unit"))
   d["space_ax"] = ["x", "y", "z"][:d["space_ax"].size]
   d = xr.merge([d, xr.DataArray(recording.get_channel_locations(axes="xy"), dims=["channel", "space_ax"], name="channel_loc", coords=dict(space_ax=["x", "y"]))])
   d["channel"] = recording.channel_ids
   d["rnd_spike_unit"] = xr.DataArray(analyzer.get_extension("random_spikes").get_random_spikes()["unit_index"], dims="rnd_spike")
   d["wf_t"] = (np.arange(d.sizes["wf_t"]) - analyzer.get_extension("waveforms").nbefore)/recording.sampling_frequency
   d["rnd_spike"] = d["random_spikes"]
   d = d.drop_vars("random_spikes")
   d["spike_unit"] = d["unit"].isel(unit=d["spike_unit_index"])
   d = d.set_coords(["rnd_spike_unit", "spike_unit", "spike_unit_index", "sparse_channel_name"])
   is_channel_in_sparse = (d["channel"] == d["sparse_channel_name"]).any("sparse_channel")
   if (is_channel_in_sparse.astype(int).sum("channel") < 1).any():
       raise Exception(f'Some units have no sparse channels...\n{(is_channel_in_sparse.astype(int).sum("channel"))}')
   d["primary_channel"] = ((d["channel_loc"] - d["unit_locations"])**2).sum("space_ax").where(is_channel_in_sparse).idxmin("channel")
   if d["primary_channel"].isnull().any():
       raise Exception("na primary channels...")
   d["unit2"] = d["unit"].to_numpy()
   return d

def compute_spike_additional_info(d: xr.Dataset, t_bins, n_amp_bins=50) -> xr.Dataset:
   progress= tqdm.tqdm(desc="spike_amp_density", total=d.sizes["unit"])
   def compute_spike_density_map(g):
      r = histogram(g["spike_time"], g["spike_amplitudes"], bins=[t_bins, n_amp_bins], density=True)
      r["spike_amplitude_bin"] = xr.DataArray(r["spike_amplitudes_bin"].to_numpy(), dims="spike_amplitudes_bin")
      r["spike_amplitudes_bin"] = np.arange(n_amp_bins)
      r = r.rename(spike_amplitudes_bin="spike_amp_bin")
      progress.update()
      return r
   
   ret = xr.Dataset()
   ret["spike_density_map"] = d[["spike_time", "spike_amplitudes"]].groupby("spike_unit").apply(compute_spike_density_map).rename(spike_unit="unit")
   ret["spike_mean_amp"] = d.set_coords("spike_time")["spike_amplitudes"].groupby(spike_unit=xr.groupers.UniqueGrouper(), spike_time=xr.groupers.BinGrouper(t_bins)
               ).mean().drop_vars("spike_time_bins").rename(spike_time_bins="spike_time_bin", spike_unit="unit")
   return ret

def compute_template_densities(d: xr.Dataset, n_amp_wf_bins) -> xr.DataArray:
   progress= tqdm.tqdm(desc="template_density_map", total=d.sizes["unit"])
   def compute_template_density_map(g):
      r = histogram(g["waveforms"],  dim=["rnd_spike"], bins=n_amp_wf_bins)
      r["waveform_amplitude_bin"] = xr.DataArray(r["waveforms_bin"].to_numpy(), dims="waveforms_bin")
      r["waveforms_bin"] = np.arange(101)
      r = r.rename(waveforms_bin="wf_amp_bin")
      progress.update()
      return r
   return d[["waveforms"]].groupby("rnd_spike_unit").apply(compute_template_density_map).rename(rnd_spike_unit="unit")

def compute_recording_samples(d: xr.Dataset, recording: si.BaseRecording, segment_duration: float, n_segments):
    min_t = d["spike_time"].min().item()
    max_t = d["spike_time"].max().item()
    t_bins = np.arange(min_t, max_t, segment_duration)
    spike_count_density_grp =  d[["spike_time"]].groupby(spike_unit=xr.groupers.UniqueGrouper(), spike_time=xr.groupers.BinGrouper(t_bins))
    spike_count_density = spike_count_density_grp.count().drop_vars("spike_time_bins").rename(spike_time_bins="spike_time_bin", spike_unit="unit")["spike_time"].fillna(0)
    sorted = xr.apply_ufunc(np.argsort, spike_count_density, input_core_dims=[["spike_time_bin"]], output_core_dims=[["spike_time_bin"]], kwargs=dict(axis=-1))
    selected = sorted.isel(spike_time_bin=slice(-n_segments, None)).rename(spike_time_bin="raw_seg")
    progress = tqdm.tqdm(total=selected.size, desc="extracting some segments")
    def get_trace(position, unit):
        pc = d["primary_channel"].sel(unit=unit).item()
        if pd.isna(pc):
            print(d["primary_channel"])
            print(d["primary_channel"].isnull())
            raise Exception(f"nan primary channel for unit {unit}")
        start_frame = int((t_bins[position]) *rec.sampling_frequency)
        end_frame = start_frame+int(segment_duration*rec.sampling_frequency)
        try:
            trace = rec.get_traces(channel_ids=[pc], start_frame=start_frame, end_frame=end_frame)[:, 0]
        except Exception:
            print(f"pc={pc}\n{d['primary_channel']}")
            raise
        coords = (np.arange(start_frame, end_frame)/rec.sampling_frequency)
        if trace.shape != coords.shape:
            trace=np.pad(trace, (0, coords.size-trace.size), constant_values=np.nan)
        progress.update()
        return trace, coords
    return xr.apply_ufunc(get_trace, selected, selected["unit"], output_core_dims=[["rec_t_index"], ["rec_t_index"]], vectorize=True)


def xarray_sorting_analysis_data(analyzer: si.SortingAnalyzer, sorting: si.BaseSorting, recording: si.BaseRecording, t_bin_secs, n_amp_bins, n_amp_wf_bins):
   d = spike_interface_to_xarray(analyzer, sorting, recording)
   max_t = recording.get_duration()
   t_bins = np.array(list(range(0, int(max_t), t_bin_secs)) + [max_t])
   d = xr.merge([d, compute_spike_additional_info(d, t_bins, n_amp_bins)])
   d["template_density"] = compute_template_densities(d, n_amp_wf_bins)
   d["raw_trace"], d["raw_trace_t"] = compute_recording_samples(d, recording, 1, 2)
   d = d.set_coords("raw_trace_t")
   return d


analyzer: si.SortingAnalyzer = si.load(a.spikeinterface_analyzer_path)
rec: si.BaseRecording = si.load(a.recording_path)
print(analyzer)
print(rec)
res_xr = xarray_sorting_analysis_data(analyzer, analyzer.sorting, rec, 30, 50, 101)
print(res_xr)
res_xr.to_zarr(a.output_path)



 
