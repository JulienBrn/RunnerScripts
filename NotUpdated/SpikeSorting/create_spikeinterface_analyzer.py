from pathlib import Path
from pydantic import Field, BaseModel
from script2runner import CLI
from typing import List, Literal, Dict, ClassVar, Annotated
import shutil, datetime as dt
import yaml
import re
import subprocess, sys
import time


start_path_patterns = ["/media/filer2/T4b/", "/media/filer2/T4/", "/media/t4user/data1/", "/media/BigNAS/", "/home/t4user/"]
def get_file_pattern_from_suffix_list(start_path_patterns, suffixes):
    def mk_or_pattern(options):
        return '(('+ ")|(".join([re.escape(opt) for opt in options])+ '))'
    return '^'+ mk_or_pattern(start_path_patterns)+r'[^\\]*'+ mk_or_pattern(suffixes) + "$"
    
class AnalyzerMainParam(BaseModel):
    max_spikes_per_unit: None | int = None

class Args(CLI):
    """
        - Goal: Creates analysis data of the recording. 
        - Technique: Uses spike interface sorting analyzer and custom tools. 
        - Formats: Input and output formats are in the .zarr formats generated by spike interface.
    """
    recording_path: Annotated[Path, Field(
        description="Path to the folder containing the (usually preprocessed) recording",
        examples=["/media/t4user/data1/Data/SpikeSorting/...si.zarr"], 
        json_schema_extra=dict(pattern=get_file_pattern_from_suffix_list(["/media/t4user/data1/"], [".si.zarr"]))
    )]
    sorting_or_analyzer_path: Annotated[Path, Field(
        description="Path to the folder containing the (usually preprocessed) recording",
        examples=["/media/t4user/data1/Data/SpikeSorting/...si.zarr"], 
        json_schema_extra=dict(pattern=get_file_pattern_from_suffix_list(start_path_patterns, [".si.zarr"]))
    )]
    output_path: Annotated[Path, Field(
        examples=["/media/filer2/T4b/Temporary/....si.zarr"], 
        description="Location of the output analyzer", 
        json_schema_extra=dict(pattern=get_file_pattern_from_suffix_list(start_path_patterns, [".si.zarr"]))
    )]

    params: Annotated[None | Path, Field(
        default=None,
        description="Configuration for the analyzer. If None, uses default configuration.",
        examples=["/media/filer2/T4b/....yaml"], 
        json_schema_extra=dict(pattern=get_file_pattern_from_suffix_list(start_path_patterns, [".yaml"]))
    )]
    params_override: Annotated[AnalyzerMainParam, Field(
        default=AnalyzerMainParam(),
        description="Overrides parameters from the configuration file or the default configuration. Only main parameters are available",
    )]
    overwrite: Literal["yes", "no"] = Field(default="no", description="Whether to overwrite and continue if output exists")
    _run_info: ClassVar = dict(conda_env="ssnew", uses_gpu=False)
    
a = Args()

import spikeinterface as si
import yaml
import numpy as np, xarray as xr, pandas as pd
from xhistogram.xarray import histogram
import tqdm.auto as tqdm

job_kwargs = {"n_jobs": 10, "chunk_duration": "1s", "progress_bar": True}

if a.params:
    with a.config_file.open("r") as f:
        config_str = f.read()
else:
    config_str= """
    random_spikes:
        method: "uniform"
        max_spikes_per_unit: 500
    waveforms:
        ms_before: 1
        ms_after: 2
    amplitude_scalings: False
    spike_locations: False
    """

params=yaml.safe_load(config_str)
for k, v in a.params_override.model_dump().items():
    if not v is None:
        params[k] = v

print(params)

if a.output_path.exists():
    if a.overwrite =="no":
        print(f"{a.output_path} already exists")
        exit(2)
    else:
        shutil.rmtree(a.output_path)
        a.output_path.parent.mkdir(exist_ok=True, parents=True)

from xhistogram.xarray import histogram
import numpy as np, xarray as xr

rec : si.BaseRecording = si.load(a.recording_path)
sorting_or_analyzer = si.load(a.sorting_or_analyzer_path)
if isinstance(sorting_or_analyzer, si.BaseSorting):
    sorting = sorting_or_analyzer
    num_spikes = sorting.to_spike_vector().size
    sorting = sorting.time_slice(0, rec.get_duration())
    new_num_spikes = sorting.to_spike_vector().size
    if new_num_spikes != num_spikes:
        print(f"Slicing the sorting to the size of the recording changed the number of spikes from {num_spikes} to {new_num_spikes}")
    analyzer : si.SortingAnalyzer = si.create_sorting_analyzer(sorting, rec)
elif isinstance(sorting_or_analyzer, si.SortingAnalyzer):
    analyzer : si.SortingAnalyzer = sorting_or_analyzer.copy()
    analyzer.set_temporary_recording(rec)
    sorting=analyzer.sorting
else:
    raise Exception("Wrong type")
print(rec)
print(sorting)
print(analyzer)

def get_extensions_to_compute(analyzer, real_params):
    extensions_to_compute = {}
    for k in analyzer.get_computable_extensions():
        if not k in real_params:
            if analyzer.has_extension(k):
                analyzer.delete_extension(k)
        elif not analyzer.has_extension(k):
            extensions_to_compute[k] = real_params[k]
        elif real_params[k] != analyzer.get_extension(k).params:
            diff = {p:(real_params[k][p], analyzer.get_extension(k).params.get(p, None)) for p in real_params[k] if not real_params[k][p] is None}
            diff = {k:(v1, v2) for k, (v1, v2) in diff.items() if v1!=v2 and not v2 is None}
            if len(diff) > 0:
                extensions_to_compute[k] = real_params[k]
    return extensions_to_compute

real_params = {k:(analyzer.get_default_extension_params(k) | params.get(k, {}))  
            for k in analyzer.get_computable_extensions() if params.get(k, {}) != False}
print(f"Real params\n{real_params}")
extensions_to_compute = dict(toto=3)
while len(extensions_to_compute) > 0:
    prev_extension_to_compute = extensions_to_compute
    extensions_to_compute = get_extensions_to_compute(analyzer, real_params)
    if prev_extension_to_compute == extensions_to_compute: 
        break
    print(f"Extensions to compute\n{extensions_to_compute}")
    analyzer.compute_several_extensions(extensions_to_compute)
if len(extensions_to_compute) > 0:
    print(f"Some extensions were not computed...\n{extensions_to_compute}")
print(analyzer)

analyzer.save_as(folder=a.output_path, format="zarr")


 
