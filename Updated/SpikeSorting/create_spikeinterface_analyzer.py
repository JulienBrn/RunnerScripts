from pathlib import Path
from pydantic import Field, BaseModel
from script2runner import CLI
from typing import List, Literal, Dict, ClassVar, Annotated

from dafn.runner_helper import get_file_pattern_from_suffix_list, check_output_paths
    
class AnalyzerMainParam(BaseModel):
    max_spikes_per_unit: None | int = None

class Args(CLI):
    """
        - Goal: Creates analysis data of the recording. 
        - Technique: Uses spike interface sorting analyzer and custom tools. 
        - Formats: Input and output formats are in the .zarr formats generated by spike interface.
    """
    recording_path: Annotated[Path, Field(
        description="Path to the folder containing the (usually preprocessed) recording",
        default="/media/t4user/data1/Data/SpikeSorting/...si.zarr", 
        json_schema_extra=dict(pattern=get_file_pattern_from_suffix_list([".si.zarr"]))
    )]
    sorting_path: Annotated[Path, Field(
        description="Path to the folder containing the (usually preprocessed) recording",
        default="/media/t4user/data1/Data/SpikeSorting/...si.zarr", 
        json_schema_extra=dict(pattern=get_file_pattern_from_suffix_list([".si.zarr"]))
    )]
    output_path: Annotated[Path, Field(
        default="/media/filer2/T4b/Temporary/....si.zarr", 
        description="Location of the output analyzer", 
        json_schema_extra=dict(pattern=get_file_pattern_from_suffix_list([".si.zarr"]))
    )]
    params: Annotated[None | Path, Field(
        default=None,
        description="Configuration for the analyzer. If None, uses default configuration.",
        json_schema_extra=dict(pattern=get_file_pattern_from_suffix_list([".yaml"]))
    )]
    params_override: Annotated[AnalyzerMainParam, Field(
        default=AnalyzerMainParam(),
        description="Overrides parameters from the configuration file or the default configuration. Only main parameters are available",
    )]
    allow_output_overwrite: bool = Field(default=False, description="If yes, erases the outputs if they exists before starting computation")
    _run_info: ClassVar = dict(conda_env="ss", gpu=False, cpu=10.0, memory=10)
    
args = Args()

import spikeinterface as si
import yaml
from dafn.spike_sorting import default_analyzer_params, create_spikeinterface_analyzer

with check_output_paths(args.output_path, args.allow_output_overwrite) as output_path:
    if args.params:
        with args.config_file.open("r") as f:
            params = yaml.safe_load(f)
    else:
        params = default_analyzer_params

    for k, v in args.params_override.model_dump().items():
        if not v is None:
            params[k] = v
    rec : si.BaseRecording = si.load(args.recording_path)
    sorting = si.load(args.sorting_path)
    analyzer = create_spikeinterface_analyzer(rec, sorting, params, {"n_jobs": 10,"chunk_duration": "1s","progress_bar": True})
    analyzer.save_as(folder=output_path, format="zarr")
    

# params=yaml.safe_load(config_str)
# for k, v in args.params_override.model_dump().items():
#     if not v is None:
#         params[k] = v

# print(params)

# if a.output_path.exists():
#     if a.overwrite =="no":
#         print(f"{a.output_path} already exists")
#         exit(2)
#     else:
#         shutil.rmtree(a.output_path)
#         a.output_path.parent.mkdir(exist_ok=True, parents=True)

# from xhistogram.xarray import histogram
# import numpy as np, xarray as xr

# rec : si.BaseRecording = si.load(a.recording_path)
# sorting_or_analyzer = si.load(a.sorting_or_analyzer_path)
# if isinstance(sorting_or_analyzer, si.BaseSorting):
#     sorting = sorting_or_analyzer
#     num_spikes = sorting.to_spike_vector().size
#     sorting = sorting.time_slice(0, rec.get_duration())
#     new_num_spikes = sorting.to_spike_vector().size
#     if new_num_spikes != num_spikes:
#         print(f"Slicing the sorting to the size of the recording changed the number of spikes from {num_spikes} to {new_num_spikes}")
#     analyzer : si.SortingAnalyzer = si.create_sorting_analyzer(sorting, rec)
# elif isinstance(sorting_or_analyzer, si.SortingAnalyzer):
#     analyzer : si.SortingAnalyzer = sorting_or_analyzer.copy()
#     analyzer.set_temporary_recording(rec)
#     sorting=analyzer.sorting
# else:
#     raise Exception("Wrong type")
# print(rec)
# print(sorting)
# print(analyzer)

# def get_extensions_to_compute(analyzer, real_params):
#     extensions_to_compute = {}
#     for k in analyzer.get_computable_extensions():
#         if not k in real_params:
#             if analyzer.has_extension(k):
#                 analyzer.delete_extension(k)
#         elif not analyzer.has_extension(k):
#             extensions_to_compute[k] = real_params[k]
#         elif real_params[k] != analyzer.get_extension(k).params:
#             diff = {p:(real_params[k][p], analyzer.get_extension(k).params.get(p, None)) for p in real_params[k] if not real_params[k][p] is None}
#             diff = {k:(v1, v2) for k, (v1, v2) in diff.items() if v1!=v2 and not v2 is None}
#             if len(diff) > 0:
#                 extensions_to_compute[k] = real_params[k]
#     return extensions_to_compute

# real_params = {k:(analyzer.get_default_extension_params(k) | params.get(k, {}))  
#             for k in analyzer.get_computable_extensions() if params.get(k, {}) != False}
# print(f"Real params\n{real_params}")
# extensions_to_compute = dict(toto=3)
# while len(extensions_to_compute) > 0:
#     prev_extension_to_compute = extensions_to_compute
#     extensions_to_compute = get_extensions_to_compute(analyzer, real_params)
#     if prev_extension_to_compute == extensions_to_compute: 
#         break
#     print(f"Extensions to compute\n{extensions_to_compute}")
#     analyzer.compute_several_extensions(extensions_to_compute)
# if len(extensions_to_compute) > 0:
#     print(f"Some extensions were not computed...\n{extensions_to_compute}")
# print(analyzer)

# analyzer.save_as(folder=a.output_path, format="zarr")


 
