from pathlib import Path
from pydantic import Field, BaseModel
from script2runner import CLI
from typing import List, Literal, Dict, ClassVar, Annotated
import re
import abc
import xarray as xr, numpy as np, pandas as pd
import dask.diagnostics
from dafn.runner_helper import get_file_pattern_from_suffix_list, check_output_paths, XarrayLoader

class AutoThresholdingMethod(BaseModel):
    threshold_method: Literal["auto"] = "auto"
    per_channel: bool = True
    dist_output_path: None | Annotated[Path, Field(
        default="/media/filer2/T4b/Temporary/....html", 
        description="Where you want your output distribution information", 
        json_schema_extra=dict(pattern=get_file_pattern_from_suffix_list([".html"]))
    )] = None
    def get_thresholds(self, arr: xr.DataArray) -> xr.DataArray:
        from dafn.conti2events import get_thresholds, make_threshold_fig
        result = get_thresholds(arr, self.per_channel, return_info=self.dist_output_path is not None)
        if self.dist_output_path is not None:
            fig = make_threshold_fig(result[1])
            fig.write_html(self.dist_output_path)
            return result[0]
        else:
            return result


class ManualThresholdingMethod(BaseModel):
    threshold_method: Literal["manual"] = "manual"
    threshold_default_value: float
    threshold_channel_value: Dict[str, float] = {}
    def get_thresholds(self, arr) -> xr.DataArray:
        def get_threshold(chan):
            return self.threshold_channel_value.get(chan, self.threshold_default_value)
        return xr.apply_ufunc(get_threshold, arr["channel"], vectorize=True)


class NormalizePreprocessMethod(BaseModel):
    preprocessing_method: Literal["normalize"] = "normalize"
    normalize_channel: str
    def preprocess(self, arr) -> xr.DataArray:
        norm = arr.sel(channel=self.normalize_channel)
        return (arr.drop_sel(channel=self.normalize_channel)-norm)/norm
    
class ArtefactsPreprocessMethod(BaseModel):
    preprocessing_method: Literal["hartefacts"] = "hartefacts"
    def preprocess(self, arr: xr.DataArray) -> xr.DataArray:
        arr = np.abs(arr)
        arr = (arr - arr.mean("t"))/arr.std("t")
        arr = arr.sum("channel")
        arr = arr.expand_dims("channel")
        arr["channel"] = xr.DataArray(["summed_channel"], dims="channel")
        return arr
    

class FilterPreprocessMethod(BaseModel):
    preprocessing_method: Literal["filter"] = "filter"
    low_freq: float | None = None
    high_freq: float | None = None
    filter_type: Literal["lowpass", "highpass", "bandpass"]

    def preprocess(self, arr: xr.DataArray) -> xr.DataArray:
        import scipy.signal
        fs = 1/arr["t"].diff("t").mean().item()
        if self.filter_type == "lowpass":
            filter = scipy.signal.butter(3, self.low_freq, btype="lowpass", output="sos", fs=fs)
        elif self.filter_type == "highpass":
            filter = scipy.signal.butter(3, self.high_freq, btype="highpass", output="sos", fs=fs)
        elif self.filter_type == "bandpass":
            filter = scipy.signal.butter(3, [self.low_freq, self.high_freq], btype="bandpass", output="sos", fs=fs)
        arr = xr.apply_ufunc(lambda a: scipy.signal.sosfiltfilt(filter, a, axis=-1), arr, input_core_dims=[["t"]], output_core_dims=[["t"]])
        return arr
    
class PolyTimestampsPreprocessMethod(BaseModel):
    preprocessing_method: Literal["poly_timestamps"] = "poly_timestamps"
    timestamps_path: Annotated[Path, Field(
        examples=["/media/filer2/T4b/Temporary/....txt"], 
        description="Where the video timestamps generated by poly are", 
        json_schema_extra=dict(pattern=get_file_pattern_from_suffix_list([".txt"]))
    )]
    def preprocess(self, arr) -> xr.DataArray:
        import numpy as np
        time_stamps = np.genfromtxt(self.timestamps_path, skip_footer=3)
        res = arr.copy()
        if np.abs(len(time_stamps) - res.sizes["t"]) > 5:
            raise Exception(f"Sizes of timestamps in correction ({len(time_stamps)}) do not match number of timestamps in data ({res.sizes['t']})")
        elif len(time_stamps) > res.sizes["t"]:
                print(f"Removing some timestamps in correction")
                time_stamps=time_stamps[:res.sizes["t"]]
        elif len(time_stamps) < res.sizes["t"]:
            print(f"Removing some timestamps in data")
            res = res.isel(t=slice(0, len(time_stamps)))
        res["t"] = time_stamps/1000
        return res
    
class Args(CLI):
    """
        # EEG/LFP Event Detection Pipeline

This script implements a **data processing pipeline for EEG/LFP event detection**.

---

## ðŸ—‚ File Loading
- Loads time-series data (EEG/LFP) from:
  - `.zarr`
  - `.h5` files  
- Uses **xarray** for dataset handling.

---

## âš™ï¸ Preprocessing Options
Supports several preprocessing methods:
- **Normalization**: Relative to a reference channel  
- **Artifact detection**: Z-scoring and channel summation  
- **High-pass filtering**: Removes low-frequency drift  
- **Timestamp alignment**: Sync with external timestamps (e.g., Poly video)  

---

## ðŸ“‰ Thresholding Methods
Two approaches:
- **Automatic**:
  - Kernel Density Estimation (KDE) + peak detection
  - Determines thresholds per channel
- **Manual**:
  - User-specified default or per-channel thresholds

---

## ðŸ”Ž Event Detection
- Compares signal against thresholds â†’ binary above/below threshold
- Detects **rising** and **falling** edges (event start & end)
- Optional filtering of events too close together (`min_distance`)
- Creates a structured event table:
  - `event_name` (channel)
  - `start` (time)
  - `duration`

---

## ðŸ’¾ Outputs
1. **Excel file** (`.xlsx`)  
   - Contains the event table (channel, start, duration)  
2. **Interactive Plotly HTML** (`.html`)  
   - Shows:
     - Raw signals per channel
     - Threshold lines
     - Highlighted event windows

---

ðŸ‘‰ **In short:**  
The script **loads EEG/LFP data, preprocesses it, applies thresholding to detect events, saves results to Excel, and generates interactive plots for inspection.**

    """
    input: Annotated[XarrayLoader, Field(
        description="How to load the data",
    )]
    preprocessing: Annotated[List[NormalizePreprocessMethod | PolyTimestampsPreprocessMethod | ArtefactsPreprocessMethod | FilterPreprocessMethod], 
                             Field(default=[NormalizePreprocessMethod(normalize_channel="background"), 
                                            PolyTimestampsPreprocessMethod(timestamps_path="/media/file2/T4b/Temporary/....txt"), 
                                            ArtefactsPreprocessMethod(), FilterPreprocessMethod(low_freq=2, filter_type="highpass")
                                            ], description="Preprocessing to apply (t_modification, normalization (df/f), ...). Please remove methods you dont need")
    ]
    threshold: Annotated[AutoThresholdingMethod | ManualThresholdingMethod, Field(
        description="How to threshold the continuous data"
    )]
    min_distance: float | None = None
    output_path: Annotated[Path, Field(
        default="/media/filer2/T4b/Temporary/....xlsx", 
        description="Where you want your output excel file", 
        json_schema_extra=dict(pattern=get_file_pattern_from_suffix_list([".xlsx"]))
    )]
    fig_output_path: Annotated[Path, Field(
        default="/media/filer2/T4b/Temporary/....html", 
        description="Where you want your output figure", 
        json_schema_extra=dict(pattern=get_file_pattern_from_suffix_list([".html"]))
    )]
    allow_output_overwrite: Literal["yes", "no"] = Field(default="no", description="Whether to overwrite and continue if output exists")
    _run_info: ClassVar = dict()

args = Args()

import plotly.express as px
import plotly.graph_objects as go
from dafn.conti2events import continuous_to_events, plot_continuous_to_events
from dafn.runner_helper import finalize_events

with check_output_paths([args.output_path, args.fig_output_path], args.allow_output_overwrite) as [output_path, fig_output_path]:
    print("Loading data, this may take a while")
    input = args.input.load()
    for preprocessing in args.preprocessing:
        print(input)
        input = preprocessing.preprocess(input)
    print(input)
    threshold = args.threshold.get_thresholds(input)
    result = continuous_to_events(input, threshold, args.min_distance)
    finalize_events(result, output_path)
    fig = plot_continuous_to_events(input, threshold, result)
    fig.write_html(fig_output_path)
    

# for output_path in [a.output_path, a.fig_output_path]:
#     if output_path is None:
#         if output_path.exists():
#             if a.overwrite =="yes":
#                 shutil.rmtree(output_path)
#             else:
#                 raise Exception(f"Output path {output_path} already exists")
#         output_path.parent.mkdir(exist_ok=True, parents=True)


# display = print
# print("Loading data, this may take a while")
# ds = xr.Dataset()
# input = a.input.load()
# for preprocessing in a.preprocessing:
#     display(input)
#     input = preprocessing.preprocess(input)
# display(input)



# print(result.groupby("event_name").size())

# if ds.sizes["t"] > 10**5:
#     plot_ds = ds.coarsen(t=int(ds.sizes["t"] / (10**5)), boundary="trim").max()
# else:
#   plot_ds = ds

# result.sort_values("start").to_excel(a.output_path, index=False)

# fig = make_subplots(rows=plot_ds["channel"].size, cols=1, shared_xaxes=True)
# for chan in range(plot_ds["channel"].size):
#   chan_ds = plot_ds.isel(channel=chan)
#   m, M = chan_ds["input"].min().item(), chan_ds["input"].max().item()
#   fig.add_trace(go.Scatter(x=chan_ds["t"], y=chan_ds["input"], name=chan_ds["channel"].item(), opacity=0.5), row=chan+1, col=1)
#   fig.add_hline(y=chan_ds["thresh"].item(), line_dash="dot",
#               annotation_text=str(chan_ds["channel"].item()), 
#               annotation_position="bottom right", row=chan+1, col=1)
#   sub_df: pd.DataFrame = result.loc[result["event_name"] == chan_ds["channel"].item()]
#   for _, row in sub_df.iterrows():
#     fig.add_trace(
#       go.Scatter(
#           x=[row["start"], row["start"] + row["duration"], row["start"] + row["duration"], row["start"]], 
#           y=[m, m, M, M],
#           fill="toself",
#           fillcolor="pink",
#           opacity=0.5,
#           mode="lines",
#           line=dict(width=0), showlegend=False), row=chan+1, col=1)
# fig.write_html(a.fig_output_path)
